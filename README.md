# ğŸ§  Stroke Prediction using Binary Classification â€“ Intelligent Systems Project

This project was developed for the **Intelligent Systems** course and applies theoretical and practical concepts learned in lab (Logistic Regression, Decision Trees, Random Forest) on a new real-world binary classification dataset â€“ `healthcare-dataset-stroke-data.csv`.

## ğŸ‘¨â€ğŸ’» Author
**Maxim Francesco** â€“ 3rd Year, Group 30234, UTCN

---

## ğŸ¯ Project Objective

The main goal was to understand and apply:

- **Logistic Regression** with regularization and polynomial expansion
- **Data preprocessing** (handling missing values, normalization, polynomial features)
- **Tree-based algorithms** (Decision Trees and Random Forest)
- **Underfitting vs Overfitting** behavior
- **Feature Importance** interpretation to understand model decision-making

---

## ğŸ“Š Code Summary

### ğŸ” Data Exploration (EDA)

- Loaded the dataset using `pandas` and checked shape, types, and target (`stroke`) distribution
- Identified class imbalance (only ~5% positive examples)
- Filled missing values in the `bmi` column using mean imputation
- Investigated feature correlation with the target

### ğŸ› ï¸ Preprocessing

- Normalized numerical features using `StandardScaler`
- Applied `PolynomialFeatures` (degree 2) to expand feature space
- Performed train/validation/test split using `train_test_split` with stratification

### ğŸ§ª Model Training

#### 1. Logistic Regression
- Tested different regularization strengths (C = 0.1, 1.0, 10.0)
- Compared base model vs polynomial expansion
- Evaluated using accuracy, recall, ROC-AUC

#### 2. Decision Tree
- Tuned `max_depth`, `min_samples_split`, `min_samples_leaf`
- Overfitting observed on deeper trees

#### 3. Random Forest
- Used `n_estimators=100` and `max_depth=7`
- Achieved best validation performance

### âœ… Evaluation

- Used `accuracy_score`, `classification_report`, `roc_auc_score`
- Plotted confusion matrices and ROC curves
- Analyzed `feature_importances_` to understand key predictors

---

## ğŸ“˜ What I Learned

- How to work with real-world, imbalanced data and handle missing values
- Importance of scaling and polynomial expansion in logistic regression
- Impact of tree depth and splitting parameters in preventing overfitting
- How Random Forests reduce variance compared to single decision trees
- How to interpret model behavior through feature importance

---

## ğŸ“¦ Tech Stack

- Python 3.10
- Jupyter Notebook
- pandas, numpy
- matplotlib, seaborn
- scikit-learn

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ Untitled4.ipynb
â”œâ”€â”€ data/
â”‚   â””â”€â”€ healthcare-dataset-stroke-data.csv
â”œâ”€â”€ README.md
```

---

## ğŸ“ˆ Sample Results (may vary)

- Logistic Regression (AUC â‰ˆ 0.78)
- Decision Tree (AUC â‰ˆ 0.75)
- Random Forest (AUC â‰ˆ 0.81)
- F1 Score for positive class â‰ˆ 0.45

---

## ğŸ“¬ Contact

Questions or suggestions?

**Email**: maaximfrancesco@gmail.com  
**LinkedIn**: [linkedin.com/in/francescomaxim](https://www.linkedin.com/in/francescomaxim)

---
